name: Build, Sync, and Deploy

on:
  push:
    branches: [main]
  workflow_dispatch:
    inputs:
      force_deploy:
        description: "Run deploy even if nothing changed"
        type: boolean
        default: false
      force_api_build:
        description: "Rebuild/push API image even if unchanged"
        type: boolean
        default: false
      force_dashboard_build:
        description: "Rebuild dashboard even if unchanged"
        type: boolean
        default: false
      force_restart:
        description: "Restart services at the end (even if no updates)"
        type: boolean
        default: false

permissions:
  contents: read
  packages: write
  id-token: write
  attestations: write

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  # General & Docker
  REGISTRY: ghcr.io
  IMAGE_OWNER: ${{ github.repository_owner }}
  API_IMAGE_NAME: api
  API_DOCKERFILE_PATH: ./apps/api/Dockerfile

  # Dashboard
  DASHBOARD_ARTIFACT_NAME: dashboard-dist
  DASHBOARD_DIST_PATH: apps/dashboard/dist

  # Deployment - Remote Server
  REMOTE_USER: adminuser
  REMOTE_TARGET_BASE_DIR: /tmp/ci
  REMOTE_RUNTIME_USER: podmanuser
  REMOTE_CONFIGS_DIR: /var/lib/containers/configs
  REMOTE_DASHBOARD_VOLUME_DIR: /var/lib/containers/volumes/caddy-dashboard/_data
  REMOTE_SYSTEMD_USER_DIR: .config/containers/systemd

jobs:
  detect-changes:
    name: Detect changes
    runs-on: ubuntu-latest
    outputs:
      api: ${{ steps.filter.outputs.api }}
      dashboard: ${{ steps.filter.outputs.dashboard }}
      infra: ${{ steps.filter.outputs.infra }}
    steps:
      - uses: actions/checkout@v4
        with: { fetch-depth: 0 }

      - id: filter
        uses: dorny/paths-filter@v3
        with:
          filters: |
            api:
              - 'apps/api/**'
              - 'apps/api/Dockerfile'
              - 'packages/**'
              - 'pnpm-lock.yaml'
              - 'pnpm-workspace.yaml'
              - 'package.json'
              - 'tsconfig.base.json'
            dashboard:
              - 'apps/dashboard/**'
              - 'packages/**'
              - 'pnpm-lock.yaml'
              - 'pnpm-workspace.yaml'
              - 'package.json'
              - 'tsconfig.base.json'
            infra:
              - 'infra/**'
              - '.github/workflows/**'

  build-api:
    name: Build and push API (only if changed)
    needs: [detect-changes]
    if: needs.detect-changes.outputs.api == 'true' || (github.event_name == 'workflow_dispatch' && inputs.force_api_build)
    runs-on: ubuntu-24.04-arm
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GHCR_PAT }}

      - name: Build and push API image
        uses: docker/build-push-action@v6
        with:
          context: .
          file: ${{ env.API_DOCKERFILE_PATH }}
          push: true
          platforms: linux/arm64
          tags: |
            ${{ env.REGISTRY }}/${{ env.IMAGE_OWNER }}/${{ env.API_IMAGE_NAME }}:prod
            ${{ env.REGISTRY }}/${{ env.IMAGE_OWNER }}/${{ env.API_IMAGE_NAME }}:sha-${{ github.sha }}
          labels: |
            org.opencontainers.image.source=${{ github.repository }}
            org.opencontainers.image.revision=${{ github.sha }}
            org.opencontainers.image.run_id=${{ github.run_id }}
          cache-from: type=gha,scope=api
          cache-to: type=gha,scope=api,mode=max

      - name: Install Cosign
        uses: sigstore/cosign-installer@v3.7.0

      - name: Sign images with Cosign (Keyless)
        env:
          COSIGN_EXPERIMENTAL: "true"
        run: |
          cosign sign --yes ${{ env.REGISTRY }}/${{ env.IMAGE_OWNER }}/${{ env.API_IMAGE_NAME }}:prod

  build-dashboard:
    name: Build dashboard (only if changed)
    needs: [detect-changes]
    if: needs.detect-changes.outputs.dashboard == 'true' || (github.event_name == 'workflow_dispatch' && inputs.force_dashboard_build)
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install pnpm
        uses: pnpm/action-setup@v4
        with:
          version: 9.12.0

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version-file: .nvmrc
          cache: pnpm

      - name: Install deps (workspace)
        run: pnpm install --frozen-lockfile

      - name: Build dashboard (and its deps)
        run: pnpm --filter @orbitcheck/dashboard... build

      - name: Upload dashboard artifact
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.DASHBOARD_ARTIFACT_NAME }}
          path: ${{ env.DASHBOARD_DIST_PATH }}
          if-no-files-found: error
          retention-days: 3

  deploy:
    name: Deploy
    needs: [detect-changes, build-api, build-dashboard]
    if: >
      always() && (
        (github.event_name == 'workflow_dispatch' && inputs.force_deploy)
        || needs.detect-changes.outputs.api == 'true'
        || needs.detect-changes.outputs.dashboard == 'true'
        || needs.detect-changes.outputs.infra == 'true'
      )
    runs-on: ubuntu-latest
    environment:
      name: production
      url: https://api.orbitcheck.io
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download dashboard artifact
        if: needs.detect-changes.outputs.dashboard == 'true'
        uses: actions/download-artifact@v4
        with:
          name: ${{ env.DASHBOARD_ARTIFACT_NAME }}
          path: ${{ env.DASHBOARD_DIST_PATH }}

      - name: Upload infra to server
        if: needs.detect-changes.outputs.infra == 'true' || (github.event_name == 'workflow_dispatch' && inputs.force_deploy)
        uses: appleboy/scp-action@v0.1.7
        with:
          host: ${{ secrets.HETZNER_HOST }}
          username: ${{ env.REMOTE_USER }}
          key: ${{ secrets.HETZNER_SSH_KEY }}
          source: "infra/**"
          target: "${{ env.REMOTE_TARGET_BASE_DIR }}/infra"
          strip_components: 1

      - name: Upload dashboard dist to server
        if: needs.detect-changes.outputs.dashboard == 'true'
        uses: appleboy/scp-action@v0.1.7
        with:
          host: ${{ secrets.HETZNER_HOST }}
          username: ${{ env.REMOTE_USER }}
          key: ${{ secrets.HETZNER_SSH_KEY }}
          source: "${{ env.DASHBOARD_DIST_PATH }}/**"
          target: "${{ env.REMOTE_TARGET_BASE_DIR }}/dashboard-dist"

      - name: Deploy files and restart services
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.HETZNER_HOST }}
          username: ${{ env.REMOTE_USER }}
          key: ${{ secrets.HETZNER_SSH_KEY }}
          script: |
            set -euo pipefail

            # Section 1: Root-level file operations (configs + optional static assets)
            echo "--- Running file tasks as root (via sudo) ---"
            if [[ "${{ needs.detect-changes.outputs.infra }}" == "true" && -d ${{ env.REMOTE_TARGET_BASE_DIR }}/infra ]]; then
              # Sync ALL infra configs (if any exist at infra/config)
              if [[ -d ${{ env.REMOTE_TARGET_BASE_DIR }}/infra/config ]]; then
                sudo mkdir -p ${{ env.REMOTE_CONFIGS_DIR }}
                sudo rsync -a --delete ${{ env.REMOTE_TARGET_BASE_DIR }}/infra/config/ ${{ env.REMOTE_CONFIGS_DIR }}/
                sudo chown -R ${{ env.REMOTE_RUNTIME_USER }}:${{ env.REMOTE_RUNTIME_USER }} ${{ env.REMOTE_CONFIGS_DIR }}
              fi
            fi

            # Keep dashboard deployment
            if [[ "${{ needs.detect-changes.outputs.dashboard }}" == "true" && -d ${{ env.REMOTE_TARGET_BASE_DIR }}/dashboard-dist ]]; then
              sudo mkdir -p ${{ env.REMOTE_DASHBOARD_VOLUME_DIR }}
              sudo rsync -a --delete ${{ env.REMOTE_TARGET_BASE_DIR }}/dashboard-dist/ ${{ env.REMOTE_DASHBOARD_VOLUME_DIR }}/
              sudo chown -R ${{ env.REMOTE_RUNTIME_USER }}:${{ env.REMOTE_RUNTIME_USER }} ${{ env.REMOTE_DASHBOARD_VOLUME_DIR }}
            fi

            # Section 2: Podmanuser-level operations (quadlets + envs + restart)
            echo "--- Running systemd tasks as ${{ env.REMOTE_RUNTIME_USER }} ---"

            # Verify Quadlet generator is installed
            if ! sudo test -x /etc/systemd/user-generators/podman-quadlet && \
               ! sudo test -x /usr/lib/systemd/user-generators/podman-quadlet; then
              echo "ERROR: Podman Quadlet generator not found!"
              exit 1
            fi

            sudo -iu ${{ env.REMOTE_RUNTIME_USER }} bash <<'EOF'
              set -euo pipefail
              export XDG_RUNTIME_DIR="/run/user/$(id -u)"

              SRC="${{ env.REMOTE_TARGET_BASE_DIR }}/infra/quadlets"
              DEST_SYS_D="$HOME/${{ env.REMOTE_SYSTEMD_USER_DIR }}"     # ~/.config/containers/systemd
              DEST_USER_CFG="$HOME/.config"                             # ~/.config

              # Make sure directories exist
              mkdir -p "$DEST_SYS_D" "$DEST_USER_CFG"

              echo "Flattening Quadlet files..."
              shopt -s nullglob globstar

              # Collect ALL .container and .pod (any depth), then copy them flattened to DEST_SYS_D
              STAGE_Q=$(mktemp -d)
              for f in "$SRC"/**/*.container "$SRC"/**/*.pod; do
                [ -f "$f" ] || continue
                cp -f "$f" "$STAGE_Q/$(basename "$f")"
              done
              rsync -a --delete "$STAGE_Q"/ "$DEST_SYS_D"/

              echo "Syncing env files..."
              STAGE_CFG=$(mktemp -d)
              for envf in "$SRC"/*/env/*.env; do
                [ -f "$envf" ] || continue
                svc="$(basename "$(dirname "$(dirname "$envf")")")" # <service>/env/<file>.env -> service name
                mkdir -p "$STAGE_CFG/$svc"
                cp -f "$envf" "$STAGE_CFG/$svc/$svc.env"
              done
              for dir in "$STAGE_CFG"/*; do
                [ -d "$dir" ] || continue
                svc="$(basename "$dir")"
                mkdir -p "$DEST_USER_CFG/$svc"
                rsync -a "$dir"/ "$DEST_USER_CFG/$svc"/
              done

              # --- Prefetch Infisical runtime secrets (for app + migrations) ---
              SERVICE="${{ env.API_IMAGE_NAME }}"            # api
              CFG_DIR="$DEST_USER_CFG/$SERVICE"              # ~/.config/api
              mkdir -p "$CFG_DIR"

              INFISICAL_DOMAIN="https://infisical.bastiat.xyz"
              INFISICAL_ENV="prod"

              # Runtime token and export (/api)
              TOKEN_FILE="$HOME/.secrets/infisical/${SERVICE}.token"
              if [[ ! -f "$TOKEN_FILE" ]]; then
                echo "ERROR: Infisical runtime token file not found at $TOKEN_FILE"
                exit 1
              fi
              if [[ ! -x /bin/infisical ]]; then
                echo "ERROR: /bin/infisical CLI not found. Install it or adjust the path."
                exit 1
              fi

              export INFISICAL_TOKEN="$(cat "$TOKEN_FILE")"
              tmp_secrets="$(mktemp)"
              /bin/infisical export \
                --domain="$INFISICAL_DOMAIN" \
                --path="/api" \
                --env="$INFISICAL_ENV" \
                --format=dotenv \
                --silent \
                --include-imports \
              > "$tmp_secrets"
              chmod 600 "$tmp_secrets"
              mv "$tmp_secrets" "$CFG_DIR/${SERVICE}.secrets.env"
              unset INFISICAL_TOKEN

              # --- DB provisioning + migrations (only when API changed or forced deploy) ---
              if [[ "${{ needs.detect-changes.outputs.api }}" == "true" || ("${{ github.event_name }}" == "workflow_dispatch" && "${{ inputs.force_deploy }}" == "true") ]]; then
                # Admin token and export (/api-infra)
                ADMIN_TOKEN_FILE="$HOME/.secrets/infisical/${SERVICE}-infra.token"
                if [[ ! -f "$ADMIN_TOKEN_FILE" ]]; then
                  echo "ERROR: Infisical admin token file not found at $ADMIN_TOKEN_FILE"
                  exit 1
                fi
                export INFISICAL_TOKEN="$(cat "$ADMIN_TOKEN_FILE")"
                tmp_admin="$(mktemp)"
                /bin/infisical export \
                  --domain="$INFISICAL_DOMAIN" \
                  --path="/api-infra" \
                  --env="$INFISICAL_ENV" \
                  --format=dotenv \
                  --silent \
                  --include-imports \
                > "$tmp_admin"
                chmod 600 "$tmp_admin"
                ADMIN_ENV="$CFG_DIR/${SERVICE}.dbadmin.env"
                mv "$tmp_admin" "$ADMIN_ENV"
                unset INFISICAL_TOKEN

                echo "--- Provisioning DB roles, DB, schema, and grants (idempotent) ---"
                podman pull docker.io/library/postgres:16-alpine || true
                podman run --rm --network host \
                  --env-file "$ADMIN_ENV" \
                  docker.io/library/postgres:16-alpine sh -s <<'SH'
                set -euo pipefail

                : "${PGHOST:?}"; : "${PGPORT:=5432}"; : "${PGADMIN_USER:?}"; : "${PGADMIN_PASSWORD:?}";
                : "${APP_DB_NAME:?}"; : "${APP_DB_SCHEMA:=public}";
                : "${MIGRATION_DB_USER:=api_migrator}"; : "${MIGRATION_DB_PASSWORD:?}";
                : "${APP_DB_USER:=api_app}"; : "${APP_DB_PASSWORD:?}";
                : "${DB_EXTENSIONS:=}";  # optional

                export PGPASSWORD="$PGADMIN_PASSWORD"

                # 1) Create roles if missing; rotate passwords
                psql "host=$PGHOST port=$PGPORT dbname=postgres user=$PGADMIN_USER" -v ON_ERROR_STOP=1 -q \
                  -v MIG_USER="$MIGRATION_DB_USER" -v APP_USER="$APP_USER" \
                  -v PASS_MIG="$MIGRATION_DB_PASSWORD" -v PASS_APP="$APP_DB_PASSWORD" <<'SQL'
                DO $$
                BEGIN
                  IF NOT EXISTS (SELECT 1 FROM pg_roles WHERE rolname = :MIG_USER) THEN
                    EXECUTE format('CREATE ROLE %I LOGIN', :MIG_USER);
                  END IF;
                  IF NOT EXISTS (SELECT 1 FROM pg_roles WHERE rolname = :APP_USER) THEN
                    EXECUTE format('CREATE ROLE %I LOGIN', :APP_USER);
                  END IF;
                END
                $$;

                ALTER ROLE :MIG_USER WITH PASSWORD :PASS_MIG;
                ALTER ROLE :APP_USER WITH PASSWORD :PASS_APP;
SQL

                # 2) Ensure database exists; set owner to migrator
                if ! psql "host=$PGHOST port=$PGPORT dbname=postgres user=$PGADMIN_USER" -tAc "SELECT 1 FROM pg_database WHERE datname = '$APP_DB_NAME'" | grep -q 1; then
                  createdb -h "$PGHOST" -p "$PGPORT" -U "$PGADMIN_USER" -O "$MIGRATION_DB_USER" "$APP_DB_NAME"
                else
                  psql "host=$PGHOST port=$PGPORT dbname=postgres user=$PGADMIN_USER" -q -v ON_ERROR_STOP=1 \
                    -c "ALTER DATABASE \"$APP_DB_NAME\" OWNER TO \"$MIGRATION_DB_USER\""
                fi

                # 3) Lock down public schema and DB-wide PUBLIC access
                psql "host=$PGHOST port=$PGPORT dbname=$APP_DB_NAME user=$PGADMIN_USER" -v ON_ERROR_STOP=1 -q <<'SQL'
                REVOKE CREATE ON SCHEMA public FROM PUBLIC;
SQL
                psql "host=$PGHOST port=$PGPORT dbname=postgres user=$PGADMIN_USER" -v ON_ERROR_STOP=1 -q \
                  -v DB_NAME="$APP_DB_NAME" <<'SQL'
                REVOKE ALL ON DATABASE :"DB_NAME" FROM PUBLIC;
SQL

                # 4) Set search_path so unqualified objects go into your schema
                psql "host=$PGHOST port=$PGPORT dbname=postgres user=$PGADMIN_USER" -v ON_ERROR_STOP=1 -q \
                  -v DB_NAME="$APP_DB_NAME" -v SCHEMA="$APP_DB_SCHEMA" \
                  -v MIG_USER="$MIGRATION_DB_USER" -v APP_USER="$APP_USER" <<'SQL'
                ALTER ROLE :"MIG_USER" IN DATABASE :"DB_NAME" SET search_path = :"SCHEMA", public;
                ALTER ROLE :"APP_USER" IN DATABASE :"DB_NAME" SET search_path = :"SCHEMA", public;
SQL

                # 5) Allow connection for migrator + app users
                psql "host=$PGHOST port=$PGPORT dbname=postgres user=$PGADMIN_USER" -q -v ON_ERROR_STOP=1 \
                  -v DB_NAME="$APP_DB_NAME" -v MIG_USER="$MIGRATION_DB_USER" -v APP_USER="$APP_USER" <<'SQL'
                GRANT CONNECT ON DATABASE :"DB_NAME" TO :"MIG_USER", :"APP_USER";
SQL

                # 6) Ensure schema exists; make migrator owner if schema != public
                psql "host=$PGHOST port=$PGPORT dbname=$APP_DB_NAME user=$PGADMIN_USER" -q -v ON_ERROR_STOP=1 \
                  -v SCHEMA="$APP_DB_SCHEMA" -v MIG_USER="$MIGRATION_DB_USER" <<'SQL'
                DO $$
                BEGIN
                  IF NOT EXISTS (SELECT 1 FROM pg_namespace WHERE nspname = :SCHEMA) THEN
                    EXECUTE format('CREATE SCHEMA %I AUTHORIZATION %I', :SCHEMA, :MIG_USER);
                  END IF;
                END
                $$;
SQL
                if [ "$APP_DB_SCHEMA" != "public" ]; then
                  psql "host=$PGHOST port=$PGPORT dbname=$APP_DB_NAME user=$PGADMIN_USER" -q -v ON_ERROR_STOP=1 \
                    -v SCHEMA="$APP_DB_SCHEMA" -v MIG_USER="$MIGRATION_DB_USER" \
                    -c 'ALTER SCHEMA :"SCHEMA" OWNER TO :"MIG_USER"'
                fi

                # 7) Grants for existing objects
                psql "host=$PGHOST port=$PGPORT dbname=$APP_DB_NAME user=$PGADMIN_USER" -q -v ON_ERROR_STOP=1 \
                  -v SCHEMA="$APP_DB_SCHEMA" -v APP_USER="$APP_USER" <<'SQL'
                GRANT USAGE ON SCHEMA :"SCHEMA" TO :"APP_USER";
                GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA :"SCHEMA" TO :"APP_USER";
                GRANT USAGE, SELECT, UPDATE ON ALL SEQUENCES IN SCHEMA :"SCHEMA" TO :"APP_USER";
SQL

                # 8) Default privileges for future objects (created by migrator)
                psql "host=$PGHOST port=$PGPORT dbname=$APP_DB_NAME user=$PGADMIN_USER" -q -v ON_ERROR_STOP=1 \
                  -v SCHEMA="$APP_DB_SCHEMA" -v MIG_USER="$MIGRATION_DB_USER" -v APP_USER="$APP_USER" <<'SQL'
                ALTER DEFAULT PRIVILEGES FOR ROLE :"MIG_USER" IN SCHEMA :"SCHEMA"
                  GRANT SELECT, INSERT, UPDATE, DELETE ON TABLES TO :"APP_USER";
                ALTER DEFAULT PRIVILEGES FOR ROLE :"MIG_USER" IN SCHEMA :"SCHEMA"
                  GRANT USAGE, SELECT, UPDATE ON SEQUENCES TO :"APP_USER";
SQL

                # 9) Optional: install extensions
                if [ -n "$DB_EXTENSIONS" ]; then
                  IFS=',' read -r -a EXT_ARR <<< "$DB_EXTENSIONS"
                  for ext in "${EXT_ARR[@]}"; do
                    ext="$(echo "$ext" | xargs)"
                    [ -z "$ext" ] && continue
                    psql "host=$PGHOST port=$PGPORT dbname=$APP_DB_NAME user=$PGADMIN_USER" -q -v ON_ERROR_STOP=1 \
                      -v EXT="$ext" -v SCHEMA="$APP_DB_SCHEMA" \
                      -c 'CREATE EXTENSION IF NOT EXISTS :"EXT" WITH SCHEMA :"SCHEMA"'
                  done
                fi
SH
                echo "--- DB provisioning done ---"

                # --- Run DB migrations (one-off container) BEFORE we stop/start services ---
                IMAGE="${{ env.REGISTRY }}/${{ env.IMAGE_OWNER }}/${{ env.API_IMAGE_NAME }}:prod"
                STATIC_ENV="$CFG_DIR/$SERVICE.env"                 # from infra sync (non-secret)
                SECRET_ENV="$CFG_DIR/${SERVICE}.secrets.env"       # from Infisical

                MIG_ENV="$(mktemp)"
                trap 'rm -f "$MIG_ENV"' EXIT
                chmod 600 "$MIG_ENV"
                [[ -f "$STATIC_ENV" ]] && cat "$STATIC_ENV" >> "$MIG_ENV"
                [[ -f "$SECRET_ENV" ]] && cat "$SECRET_ENV" >> "$MIG_ENV"

                echo "--- Running DB migrations using image: $IMAGE ---"
                podman pull "$IMAGE" || true

                ATTEMPTS=10
                until podman run --rm \
                  --env-file "$MIG_ENV" \
                  --network host \
                  "$IMAGE" sh -lc 'npm run migrate:ci'; do
                  ((ATTEMPTS--)) || { echo "ERROR: Migrations failed after retries"; exit 1; }
                  echo "Migrations failed; retrying in 3s..."
                  sleep 3
                done
                echo "--- Migrations completed ---"
              fi
              # --- End migrations block ---

              echo "--- systemd daemon-reload (user) ---"
              systemctl --user daemon-reload

              echo "Discovering units from Quadlet files we just synced..."
              POD_UNITS=()
              CNTR_UNITS=()
              for f in "$DEST_SYS_D"/*.pod; do
                [ -f "$f" ] || continue
                n="$(basename "$f" .pod)"
                POD_UNITS+=("${n}-pod.service")
              done
              for f in "$DEST_SYS_D"/*.container; do
                [ -f "$f" ] || continue
                n="$(basename "$f" .container)"
                CNTR_UNITS+=("${n}.service")
              done

              echo "Pod units:    ${POD_UNITS[*]:-<none>}"
              echo "Container units: ${CNTR_UNITS[*]:-<none>}"

              # Optionally stop existing ones that match our names
              if (( ${#POD_UNITS[@]} + ${#CNTR_UNITS[@]} > 0 )); then
                echo "--- Stopping existing matching services ---"
                PATTERN=$(printf "%s\n" "${POD_UNITS[@]}" "${CNTR_UNITS[@]}" | sed 's/\.service$//' | sort -u | paste -sd '|')
                systemctl --user list-units --all --no-legend --plain '*-pod.service' '*.service' | \
                  grep -E "($PATTERN)" | awk '{print $1}' | xargs -r systemctl --user stop || true
              fi

              # Clean up old containers/pods
              podman pod rm -fa || true
              podman rm -fa || true

              # Enable (if possible) and start pod units first, then containers
              STARTED=()

              if ((${#POD_UNITS[@]} > 0)); then
                echo "--- Enabling/Starting pods ---"
                for unit in "${POD_UNITS[@]}"; do
                  systemctl --user enable "$unit" 2>/dev/null || true
                  if ! systemctl --user start "$unit"; then
                    echo "ERROR: Failed to start $unit"
                    systemctl --user status "$unit" --no-pager || true
                    journalctl --user -u "$unit" --no-pager -n 100 || true
                  else
                    STARTED+=("$unit")
                  fi
                done
              fi

              echo "--- Enabling/Starting containers ---"
              for unit in "${CNTR_UNITS[@]}"; do
                systemctl --user enable "$unit" 2>/dev/null || true
                if ! systemctl --user start "$unit"; then
                  echo "ERROR: Failed to start $unit"
                    systemctl --user status "$unit" --no-pager || true
                    journalctl --user -u "$unit" --no-pager -n 100 || true
                  else
                    STARTED+=("$unit")
                  fi
                done

              echo "--- Final status of started units ---"
              for unit in "${STARTED[@]:-}"; do
                systemctl --user status "$unit" --no-pager || true
              done

              echo "Some debugging output for visibility:"
              echo "Files in $DEST_SYS_D:"
              ls -al "$DEST_SYS_D" || true
              echo
              echo "User unit-files (look for 'generated' entries):"
              systemctl --user list-unit-files --all | sed -n '1,200p' || true
              echo
              echo "Generator outputs:"
              ls -al "$XDG_RUNTIME_DIR/systemd/" || true
              ls -al "$XDG_RUNTIME_DIR/systemd"/generator* || true
EOF

            # Section 3: Cleanup
            echo "--- Cleaning up temporary files ---"
            sudo rm -rf ${{ env.REMOTE_TARGET_BASE_DIR }}